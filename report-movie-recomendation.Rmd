---
title: "Movie Recomendation System"
author: "Jesús Aguerri"
date: "11/9/2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
# 1. INTRODUCTION
This proyect aim to create an algoritm that works as movie recomendation system. Using a machine learning based approach we will try to build and algorihtm that, receiving data as inpout, will can predict movie ratings as ouput.
The data that we will use have been provided by the staff of the HervardX: PH125.9x Data Science: Capstone course. This dataset is composed by almost ten millions movies ratings (from 1 to 5 stars) generated by more than sixtynine thousand users.
Root Mean Square Error (RMSE) will be the value used for evaluate the algorithms. 
$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$
Our goal will be obtain the minimum RMSE possible. In consecuence, we first will explore the data traying to find questions that could help us to design our models. Later, we will check differnt models, traying to see which one minimize the RMSE.

# 2. TO GET THE DATA AND TO CREATE SETS
In this proyect we will use the following packages:
```{r pack, echo= TRUE, eval = TRUE, message = FALSE, warning = FALSE }
library(tidyverse)
library(caret)
library(data.table)
```
We will use the MovieLens 10M dataset, which is avaible online in the following links:
https://grouplens.org/datasets/movielens/10m/   http://files.grouplens.org/datasets/movielens/ml-10m.zip

To get the data and to create the sets we will use the following code, whixh is provided in the edx capstone project module:

```{r setup, include=TRUE, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE, cache=TRUE}
# Create edx set, validation set

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```
This code have created two objects:
  -edx: the train set
  -validation: the test set
We save the botch objects as rda files, this will be easier to run them later:
```{r guarda, include=TRUE, echo = TRUE, message = FALSE, warning = FALSE, eval = FALSE, cache=TRUE}
save(edx, file= "rda/edx.rda")
save(validation, file= "rda/validation.rda")
```


## 2.1 EXPLORING THE DATA
Using the functions "head()" and "str()" we can see the edx data structure
```{r head, echo=TRUE, eval=TRUE}
head(edx)
```
```{r str, echo=TRUE, eval=TRUE}
str(edx)
```
As we see this subset of the data have 6 variables in columns and 9.000.055 observations in rows. Each observations belongs to a rating given by one user to a movie. The variables that are included in the data set are:   
-"userId": user identification  
-"movieId": movie identification  
-"rating": the rate given by a user to a movie  
-"timestamp": the date and hour when the rating was done  
-"tittle": the tittle and year of the movie  
-"genre": the movie´s genre.    
Going a bit deeply, we can use summary() to see the structure of each variable
```{r sumary, echo=TRUE, eval=TRUE, cache=TRUE}
summary(edx)
```
And we also can see the number of unique movies and users in the  dataset
```{r unique, echo=TRUE, eval=TRUE}
edx %>% summarise(n_movies = n_distinct(movieId),
                  n_users = n_distinct(userId))
```

# 3. ANALYSIS  
We is going to explore some characteristcis of the data. Our goal is to find bias that could help us to adjust or algorihtm.  

## 3.1. Rating Distribution  
Exploring the rating distribution we can see that user gave high ratings more often than low rating, and aslo can see that half ratings are less commom that whole star ratings.
```{r distrat, echo=TRUE, eval=TRUE}
edx %>% group_by(rating) %>% summarise(count = n()) %>% arrange(desc(count))
```
Using the function "mean()" we can get the mean rating is around 3,5 starts. As we´ll see later, to use the mean to predict rating is the simplest approach posible to predict ratings.  

## 3.2. Ratings per movie  
There are movies with a lot of ratings
```{r higdistrat, echo=TRUE, eval=TRUE}
edx %>% group_by(title) %>% summarise(count= n()) %>% arrange(desc(count))
```
However, there are movies even with just one rating.
```{r lowdistrat, echo=TRUE, eval=TRUE}
edx %>% group_by(title) %>% summarise(count= n()) %>% arrange(count)
```
The following plot is usefull to visualize that question
```{r visratdis, echo=TRUE, eval=TRUE}
edx %>% 
  count(movieId) %>%
  ggplot(aes(n))+
  geom_histogram(bins = 50, color = "black") +
  scale_x_log10() +
  ggtitle("Movies(grouped) by rating frequency") +
  xlab("nº ratings")+
  ylab("movies")
```

## 3.3. Ratings per user  
We also can see that there are user that are more actives than others
```{r visratus, echo=TRUE, eval=TRUE}
edx %>%
  count(userId) %>%
  ggplot(aes(n))+
  geom_histogram(bins = 50, color = "black")+
  scale_x_log10() +
  ggtitle("Number of ratings given by users")+
  xlab("nºratings")+
  ylab("users") 
```
In addition, there are user who give better puntuations and other who are more critical
```{r visratusnot, echo=TRUE, eval=TRUE}
edx %>%
  group_by(userId) %>%
  filter(n() >= 20) %>% #just take user with more tahn 20 ratings
  summarize(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, color = "black") +
  xlab("Mean rating") +
  ylab("users") +
  ggtitle("Mean movie ratings by users")
```
  
# 4. Methods: modeling approach  
As we already say we are going to use the Root Mean Square Error (RMSE) as value to evaluate the algorithm performance. Acording to the information that we have extract during the data exploration, we are goint to try to develop diferent algorithms.  This algorithms will take into acount the bias that we are detected previously  and they will try to minimize the RMSE.
The following function will be used to compute the RMSE:
```{r rmse}
RMSE <- function(true_ratings, predicted_ratings){
 sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

## 4.1. Just the average  
The simpliest approach is to use the rating average extracted from our train set and later predict this rating for all the ratings in the test set.
```{r mu, echo=TRUE, eval=TRUE}
mu <- mean(edx$rating)
mu
mu_rmse <- RMSE(validation$rating, mu)
```
This approach predict the same rating for al the movies and variation between movies are explained as random variations represented by $\epsilon_{u,i}$  
$$ Y_{u, i} = \mu + \epsilon_{u, i} $$
We can see that the RMSE is quite high 
```{r murmse, echo=TRUE, eval=TRUE}
rmse_results <- tibble(method = "Just Average",
                           RMSE = mu_rmse)
rmse_results %>% knitr::kable()
```

## 4.2. Movie effect model  
However not all the movies are rated with the same rates. As we already see we can talk about a movie bias $b_{i}$. This parameter and its predictions could be calculated using a liner model, but this operation could send a lot of time because of the size of the data. Nevertheless we can compute this $b_{i}$ with the following code:
```{r model2, echo=TRUE, eval=TRUE}
movie_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
```
Using this parameter we will predict the ratings using the following formula:
 $$Y_{u, i} = \mu +b_{i}+ \epsilon_{u, i}$$
 Our predictions have improved but we already have to take into account other bias.
```{r model2pred, echo=TRUE, eval=TRUE}
predicted_ratings <- mu + validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_i
model_2_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie Effect Model",
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()
``` 

## 4.3. Movie effect plus user efect model 
As ther are variability across movies, there are variability across users as well. There ara user who hates every movie that they see, and other who loves all the movies. This idea can be included in our model using the parameter $b_{u}$, which represents the user bias. This parameter can be calculated with the following code:
```{r model3, echo=TRUE, eval=TRUE}
user_avgs <- edx %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
```

So we can do our predictions using this formula:
$$Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}$$
To use this formula to predict rating able us to improve our predictions:

```{r model3pred, echo=TRUE, eval=TRUE}
predicted_ratings <- validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred
model_3_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                         tibble(method="Movie + User Effects Model",  
                                     RMSE = model_3_rmse ))
rmse_results %>% knitr::kable()
``` 

## 4. Movie and user effects regularizated model   
As we have already seen in the data exploration some users create a lot of rating and others just a few. In addition, some movies are rated a lot of times and other less times. This have produced some deviations in our parameters $b_{u}$ and $b_{i}$. However this deviations can be corrected using regularization, a proces which able us to penalize movies and users with a small number of ratings.
Regularization requires to choose a tunning parameter named lambda. 
But we can´t calculate the best lambda using the validation set (the validation set have to be used to test the lambda previously choosen), so are going to create a partition into edx named train set and pick the lambda that minimize the RMSE.  
We will calculate lambda using "repeated random sub-sampling validation", also know as Monte Carlo crossvalidation, so we are going to replicate the proces five times. The final lambda will be the average of the 5 lambdas calculated.

```{r partition, echo=TRUE, eval=TRUE}
edx_l <- edx %>% select(userId, movieId, rating)
lambdas <- seq(0, 10, 0.25) # test differnt lambdas
set.seed(1)
# NOTE: This proces could take a few minutes
best_lambda <- replicate (5, simplify = FALSE, {
  test_index_l <- createDataPartition(edx_l$rating, times = 1, p = .2, list = F)
  # Create the index
  train <- edx[-test_index_l, ] # Create Train set
  test <- edx[test_index_l, ] # Create Test set
  test <- test %>% 
    semi_join(train, by = "movieId") %>%# The same movieId and usersId appears in both set
    semi_join(train, by = "userId")
  #Now, we pick the lambda
  rmses_l <- sapply(lambdas, function(l){
    mu <- mean(train$rating)
    b_i <- train %>% 
      group_by(movieId) %>%
      summarize(b_i = sum(rating - mu)/(n()+l))
    b_u <- train %>% 
      left_join(b_i, by="movieId") %>%
      group_by(userId) %>%
      summarize(b_u = sum(rating - b_i - mu)/(n()+l))
    predicted_ratings <- test %>%
      left_join(b_i, by = "movieId") %>%
      left_join(b_u, by = "userId") %>%
      mutate(pred = mu + b_i + b_u) %>%
      pull(pred)
    return(RMSE(predicted_ratings, test$rating))
  })
  lambdas[which.min(rmses_l)]
})
``` 

We have to choose the lambda that will minimize the RMSE in the validation set, so we will use the mean of the lambdas calculated

```{r model4VISlambda, echo=TRUE, eval=TRUE}
true_best_lambda <- mean(as.numeric(best_lambda)) #the average of lambdas
true_best_lambda
```  

And finally, we can predict the ratings using the validation set and show the resoults of our final model:

```{r model4FINAL, echo=TRUE, eval=TRUE}
rmses_l <- sapply(true_best_lambda, function(l){
  mu <- mean(edx$rating)
  b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- edx %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  predicted_ratings <- 
    validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, validation$rating))
})

#Predict and showing the resoults
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Regularized Movie + User Effect Model",  
                                     RMSE = min(rmses_l)))
rmse_results %>% knitr::kable()
```  
 
# 5. CONCLUSIONS

Finally we have created a model that can predict movies rating with a RMSE under 0.865, so we can predict movies rating with less than one star of error. It is necessary to highlight that this have been obtained using as variables just the raing, the user and the movie. In the future, other analysis could be done using other varaibles such as the year of the movie or the genre. These analysys could improve the resoults and to get RMSEs even lowers.
 
 
